{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 33: Quiz 3 Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference    \n",
    "Lecture video: https://www.youtube.com/watch?v=HgC1l_6ySkc             \n",
    "Chinese note: https://nbviewer.jupyter.org/github/zlotus/notes-linear-algebra/blob/master/chapter33.ipynb   \n",
    "Official summary: https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/positive-definite-matrices-and-applications/exam-3-review/MIT18_06SCF11_Ses3.9sum.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main topics to be reviewed:\n",
    "* Differential equation $\\frac{\\mathrm{d} u}{\\mathrm{~d} t}=A u$, expoentials $e^{At}$;\n",
    "* Properties of symmetric matrices: $A = A^{\\top}$.\n",
    "    * Eigenvalues are real\n",
    "    * \"Enough\" independent eigenvectors (May be can say: the algebraic multiplicity is equal to geomotric multiplicity for every eigenvalue ) \n",
    "    * Always diagonalizable\n",
    "    * Eigenvectors are orthogonal thus $A=S\\Lambda S^{-1} = Q \\Lambda Q^{\\top}$\n",
    "* Singular value decomposition (SVD): $A=U \\Sigma V^{T}$. $A$ can be any matrix. \n",
    "\n",
    "Now we will review the above main topics through example problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Solve $\\frac{\\mathrm{d} u}{\\mathrm{~d} t}=A u=\\left[\\begin{array}{ccc}0 & -1 & 0 \\\\ 1 & 0 & -1 \\\\ 0 & 1 & 0\\end{array}\\right] u$\n",
    "\n",
    "The general solution to this equation will look like:\n",
    "$$\\mathbf{u}(t)=c_{1} e^{\\lambda_{1} t} \\mathbf{x}_{1}+c_{2} e^{\\lambda_{2} t} \\mathbf{x}_{2}+c_{e} e^{\\lambda_{3} t} \\mathbf{x}_{3}$$\n",
    "\n",
    "* What are the eigenvalues of $A$?  \n",
    "The matrix $A$ is singular thus $\\lambda_1 = 0$. This is an anti-symmetric or skew-symmetric matrix, its eigenvalues will be imaginary. We obtain the eigenvalues by solving  $\\operatorname{det}(A-\\lambda I)=0$: \n",
    "\n",
    "$$\\left[\\begin{array}{ccc}-\\lambda & -1 & 0 \\\\ 1 & -\\lambda & -1 \\\\ 0 & 1 & \\lambda\\end{array}\\right]=\\lambda^{3}+2 \\lambda=0, \\lambda_{2}=\\sqrt{2} i, \\lambda_{3}=-\\sqrt{2} i$$\n",
    "\n",
    "The solution will look like:\n",
    "$$\\mathbf{u}(t)=c_{1} \\mathbf{x}_{1}+c_{2} e^{\\sqrt{2} i t} \\mathbf{x}_{2}+c_{e} e^{-\\sqrt{2} i t} \\mathbf{x}_{3}$$\n",
    "\n",
    "The solution does not diverge or converge, it is periodic. \n",
    "\n",
    "* When does it return to its original value?(What is the period?)\n",
    "$u(0) = c_1 + c_2 + c_3$. We want $u(T)= u(0)$. Thus $u(T) = c_1 + c_2 + c_3 $, which exactly happen when $e^{\\sqrt{2} i t}=e^{0} \\Longrightarrow \\sqrt{2} t=2 \\pi$. Therefore, $t=\\pi \\sqrt{2}$\n",
    "\n",
    "* Show that two eigenvectors of $A$ are orthogonal.   \n",
    "The eigenvectors of symmetric matix or skew symmetric matrix are always orthogonal. One choice of eigenvectors of $A$ is:\n",
    "$$\\mathbf{x}_{1}=\\left[\\begin{array}{l}1 \\\\ 0 \\\\ 1\\end{array}\\right], \\mathbf{x}_{2}=\\left[\\begin{array}{r}-1 \\\\ \\sqrt{2} i \\\\ 1\\end{array}\\right], \\mathbf{x}_{3}=\\left[\\begin{array}{r}1 \\\\ \\sqrt{2} i \\\\ -1\\end{array}\\right]$$\n",
    "    * Note: What's the condition that makes the eigenvectors of matrix orthogonal? $AA^{\\top} = A^{\\top}A$. Symmetrix matrix ($A=A^{\\top}$), anti-symmetrix matrix ($A=-A^{\\top}$), and orthogonal matrix ($Q^{-1}=Q^{\\top}$)all satisfies this condition. Thus the eigenvectors of these matrix are all orthogonal. \n",
    "    \n",
    "* The solution to this differential equation is $\\mathbf{u}(t)=e^{A t} \\mathbf{u}(0)$. How would we compute $e^{At}$? \n",
    "$$A=S \\Lambda S^{-1}, e^{A t}=S e^{\\Lambda t} S^{-1}=S\\left[\\begin{array}{llll}\n",
    "e^{\\lambda_{1} t} & & & \\\\\n",
    "& e^{\\lambda_{1} t} & & \\\\\n",
    "& & \\ddots & \\\\\n",
    "& & & & \\\\\n",
    "& & & & e^{\\lambda_{1} t}\n",
    "\\end{array}\\right] S^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Suppose we know the eigenvalues of matrix are $\\lambda_{1}=0, \\lambda_{2}=c, \\lambda_{3}=2$, eigenvectors are: $x_{1}=\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 1\\end{array}\\right], x_{2}=\\left[\\begin{array}{l}1 \\\\ -1 \\\\ 0\\end{array}\\right], x_{3}=\\left[\\begin{array}{c}1 \\\\ 1 \\\\ -2\\end{array}\\right]$.\n",
    "\n",
    "* For which $c$ is the matrix diagonalizable?  \n",
    "The condition of matrix being diagonalizable is 'enough' independent eigenvectors. Obviously, $x_1, x_2, x_3 $ are already independent. Thus for all $c$, the matrix is diagonalizable. \n",
    "* For which values of $c$ is the matrix symmetric?  \n",
    "Because the eigenvalues of symmetric matrix are real, thus $c$ should be real.  \n",
    "* For which values of $c$ is the matrix positive definite?  \n",
    "All the positive definite are symmetric and their eigenvalues are positive. Because $\\lambda_1=0$, any $c$ can not make the matrix positive definite.  \n",
    "* Is it a Markov matrix?  \n",
    "In a Markov matrix, one of eigenvalues must be 1 and the others are smaller than 1. Because $\\lambda_{3} =2$, this can not be a Markov matrix for any $c$  \n",
    "* Could $P = \\frac{1}{2}A$ be a projection matrix?   \n",
    "Projection matrix is symmetric and real. Thus their eigenvalues are real. Also, we know that $$P^2 = P \\Longrightarrow PPv=Pv \\Longrightarrow \\lambda Pv = \\lambda v \\Longrightarrow \\lambda^2 v = \\lambda v \\Longrightarrow \\lambda^2  = \\lambda $$\n",
    "Thus $\\lambda = 0 or 1$. If $c= 0 or 2$, then $\\frac{1}{2}A$ can be a projection matrix. \n",
    "\n",
    "Note: The eigenvectors are orthogonal in the problem setting, that is the base for $A$ to possibly be symmetric, positive definite, and projection matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD for any matrix $A$,  $A=U \\Sigma V^{T}$. Let's first review how to do SVD.   \n",
    "First we compute $V$: $A^{T} A=V \\Sigma^{T} U^{T} U \\Sigma V^{T}=V\\left(\\Sigma^{T} \\Sigma\\right) V^{T}$. $V$ is the eigenvector matrix for $A^{\\top}A$, $\\Sigma^{T} \\Sigma$ is the eigenvalue matrix of $A^{\\top}A$.  \n",
    "Then we compute $U$: $A A^{T}=U \\Sigma^{T} V^{T} V \\Sigma U^{T}=U\\left(\\Sigma^{T} \\Sigma\\right) U^{T}$, but we should be careful that we may introduce a sign error. We can use $A v_{i}=\\sigma_{i} u_{i}$ to calculate $U$ from $V$. \n",
    "\n",
    "Now let's see the example question.  \n",
    "3.Suppose $\\Sigma=\\left[\\begin{array}{ll}3 & 0 \\\\ 0 & 2\\end{array}\\right]$ and $U$ and $V$ each have two columns.   \n",
    "* What can we say about $A$ ?   \n",
    "From $\\Sigma$, we can tell that $A$ is a $2\\times 2$ matrix and it has no zero singular value. This tells us that $A$ is nonsingular.  \n",
    "* What if $\\Sigma=\\left[\\begin{array}{rr}3 & 0 \\\\ 0 & -5\\end{array}\\right]$?  \n",
    "This is not possible because singular value can not be negative.  \n",
    "* What if $\\Sigma=\\left[\\begin{array}{rr}3 & 0 \\\\ 0 & 0\\end{array}\\right]$?  \n",
    "Then $A$ is a singular matrix with rank of 1. Its null space dimension is 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.We're told that $A$ is symmetric and orthogonal. \n",
    "* What can we say about its eigenvalues? \n",
    "The eigenvalues of symmetric matrix are real. The eigenvalues of orthogonal matrices $Q$ have $\\mid \\lambda \\mid=1$ because \n",
    "$$ Q x = \\lambda x \\Longrightarrow \\|Q x\\|=|\\lambda| \\| x \\mid \\Longrightarrow |x\\|=|\\lambda|\\| x| \\Longrightarrow \\mid \\lambda \\mid=1$$  \n",
    "* True or False: $A$ is sure to be positive definite.  \n",
    "False. Because it is possible that $\\lambda = -1$, but the eigenvalue of positive definite matrix can not be negative. \n",
    "\n",
    "* True or False: $A$ has no repeated eigenvalues.  \n",
    "False. If $A$'s dimension is larger than 2, there must be repreated eigenvalues. \n",
    "\n",
    "* Is $A$ diagonalizable?   \n",
    "Yes. All symmetric and orthogonal matrices can be diagonalized. \n",
    "\n",
    "* Is $A$ nonsingular?  \n",
    "Yes. All orthogonal matrices are all nonsingular because their eigenvalues can not be 0. \n",
    "\n",
    "* Prove: $P = \\frac{1}{2}(A+I)$ is projection matrix.\n",
    "\n",
    "Projection matrix is symmetric: Because $A, I$ are symmetric thus $P$ is also symmetric.  \n",
    "Projection matrix satisfies: $P^2=P$: $\\frac{1}{4}\\left(\\underbrace{A^{2}}_{I}+2 A+I\\right)=\\frac{1}{2}(A+I)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
