{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 17: Orthogonal Matrices and Gram-Schmidt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference    \n",
    "Lecture video: https://www.youtube.com/watch?v=0MtwqhIwdrI             \n",
    "Chinese note: https://nbviewer.jupyter.org/github/zlotus/notes-linear-algebra/blob/master/chapter17.ipynb  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define orthonormal vectors: $q_{i}^{T} q_{j}=\\left\\{\\begin{array}{ll}0 & i \\neq j \\\\ 1 & i=j\\end{array}\\right.$\n",
    "\n",
    "Then we use orthonormal vectors as the column vectors of matrix, $Q=\\left[q_{1} q_{2} \\cdots q_{n}\\right]$. $Q$ is called orthonormal matrix. \n",
    "\n",
    "For orthonormal matrix $Q$, we have:\n",
    "\n",
    "$$\n",
    "Q^{T} Q=\\left[\\begin{array}{c}\n",
    "q_{1}^{T} \\\\\n",
    "q_{2}^{T} \\\\\n",
    "\\vdots \\\\\n",
    "q_{n}^{T}\n",
    "\\end{array}\\right]\\left[q_{1} q_{2} \\cdots q_{n}\\right] = \\left[\\begin{array}{cccc}\n",
    "1 & 0 & \\cdots & 0 \\\\\n",
    "0 & 1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & 1\n",
    "\\end{array}\\right]=I\n",
    "$$\n",
    "\n",
    "Specially, if $Q$ is square matrix, we can know that $Q$ is invertible because its columns are orthonormal. Then $Q^{T} Q=I \\Longrightarrow Q^{\\top} = Q^{-1}$. \n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "* A permutation matrix: $Q=\\left[\\begin{array}{lll}0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1\\end{array}\\right]$, $Q^{T}=\\left[\\begin{array}{lll}0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0\\end{array}\\right]$ $\\Longrightarrow Q^{\\top}Q = I$\n",
    "* An example mentioned in last lecture: $Q=\\left[\\begin{array}{cc}\\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta\\end{array}\\right]$, the length of each column vector is 1 and the column vectors are orthogonal to each other. \n",
    "* $Q=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right]$, the length of each column vector is 1 and the column vectors are orthogonal to each other. \n",
    "* Using the above matrix $Q=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}1 & 1 \\\\ 1 & -1\\end{array}\\right]$, we can construct a new orthonormal matrix $Q^{\\prime}=c\\left[\\begin{array}{cc}Q & Q \\\\ Q & -Q\\end{array}\\right] = \\frac{1}{2}\\left[\\begin{array}{cccc}1 & 1 & 1 & 1 \\\\ 1 & -1 & 1 & -1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & -1 & 1\\end{array}\\right]$. This kind of construsting way is named after Adhermar, and it is useful to construct $2,4, 16, 64, ... $ dimensional square matrix. \n",
    "* $Q=\\frac{1}{3}\\left[\\begin{array}{ccc}1 & -2 & 2 \\\\ 2 & -1 & -2 \\\\ 2 & 2 & 1\\end{array}\\right]$, the length of each column vector is 1 and the column vectors are orthogonal to each other. \n",
    "\n",
    "Next, let's see how the orthonormal matrix makes things easy. \n",
    "\n",
    "Suppose we want to do projection, projecting vector $b$ onto the column space of orthonormal matrix $Q$. According to the last lecture, the projection matrix is: $P=Q\\left( \\underbrace{Q^{T} Q}_{I}\\right)^{-1} Q^{T} \\Longrightarrow P=Q Q^{T}$. Moreover, if $Q$ is also square matrix, then $P = Q Q^{T} = I$. In this situation, the column space of $Q$ is the whole vector space. Thus, projecting matrix is just identity matrix.  We can also test the two important properties of the projection matrix when $P = Q^{\\top}$:\n",
    "* $P^{\\top}=P: \\left(Q Q^{T}\\right)^{T}=\\left(Q^{T}\\right)^{T} Q^{T}=Q Q^{T}$\n",
    "* $P^2 = P: \\left(Q Q^{T}\\right)^{2}=Q Q^{T} Q Q^{T}=Q\\left(Q^{T} Q\\right) Q^{T}=Q Q^{T}$. \n",
    "\n",
    "Previously, we need to solve $A^{T} A \\hat{x}=A^{T} b$, now we only need to solve $ Q^{T} Q \\hat{x}=Q^{T} b \\Longrightarrow \\hat{x}=Q^{\\top}b, \\hat{x_i} = q_i^{\\top}b$. The meaning is that: when we know the orthonormal basis, the $i$th component of $\\hat{x}$ is $i$th basis vector multiplying $b$, the projection onto the $i$th basis vector is $q_i^{\\top}b$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gram-Schmidt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of Gram-Schmidt is to making linearly independent vectors orthonormal. \n",
    "Let's first consider the two vector case. There are two linearly independent vectors $a, b$ orthonormal. We first make them orthogonal vectors $A, B$, then normalize them making them orthonormal $q_{1}=\\frac{A}{\\|A\\|}, q_{2}=\\frac{B}{\\|B\\|}$\n",
    "\n",
    "* First, we let $a = A$\n",
    "* Second, we project $b$ to $a$, but here the part we want is $e=b-p$, which is perpendicular to $a$. then $B = b - \\frac{A^{T} b}{A^{T} A} A$. Let's see if $A\\perp B$:\n",
    "    $$\n",
    "A^{T} B=A^{T} b-A^{T} \\frac{A^{T} b}{A^{T} A} A=A^{T} b- A^{T}  \\frac{A^{T} A}{A^{T} A} b=0, \\underbrace{\\frac{A^{T} b}{A^{T} A}}_{\\hat{x}} A = p  \n",
    "$$\n",
    "\n",
    "* Normalizing $A, B$: \n",
    "$$\n",
    "q_{1}=\\frac{A}{\\|A\\|}, q_{2}=\\frac{B}{\\|B\\|}\n",
    "$$\n",
    "\n",
    "Now, let's consider the three vector case, $a, b, c$.\n",
    "* By following the steps of two vector case, we have $A, B$. \n",
    "* Similarly, substracting the components in $A, B$ directions from $c$ gives us $C$:\n",
    "    $$\n",
    "C=c-\\frac{A^{T} c}{A^{T} A} A-\\frac{B^{T} c}{B^{T} B} B\n",
    "$$\n",
    "* Normalizing $A,B, C$: \n",
    "\n",
    "$$\n",
    "q_{1}=\\frac{A}{\\|A\\|}, q_{2}=\\frac{B}{\\|B\\|}, q_{3}=\\frac{C}{\\|C\\|}\n",
    "$$\n",
    "\n",
    "Now let's take an example: $\\boldsymbol{a}=\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 1\\end{array}\\right], b=\\left[\\begin{array}{l}1 \\\\ 0 \\\\ 2\\end{array}\\right]$\n",
    "\n",
    "* $A=\\boldsymbol{a}=\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 1\\end{array}\\right]$\n",
    "* $B = a - \\underbrace{\\frac{A^{T} b}{A^{T} A}}_{\\frac{3}{3}} A =\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 1\\end{array}\\right]-\\frac{3}{3}\\left[\\begin{array}{l}1 \\\\ 0 \\\\ 2\\end{array}\\right]=\\left[\\begin{array}{c}0 \\\\ -1 \\\\ 1\\end{array}\\right], A\\cdot B = 0\\Longleftrightarrow A\\perp B$\n",
    "* Normalization, $q_{1}=\\frac{1}{\\sqrt{3}}\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 1\\end{array}\\right], \\quad q_{2}=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{l}0 \\\\ -1 \\\\ 1\\end{array}\\right]$. Now we have orthonormal matrix $Q=\\left[\\begin{array}{cc}\\frac{1}{\\sqrt{3}} & 0 \\\\ \\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\end{array}\\right]$. Comparing with previous $D=\\left[\\begin{array}{ll}1 & 1 \\\\ 1 & 0 \\\\ 1 & 2\\end{array}\\right]$, we can find that $Q, D$ have the same column space, we just make the previous basis orthonormal.  \n",
    "\n",
    "We have looked elimination in a matrix language: $A = LU$. Similarly, we can also using matrix language for Gram-Schmidt $A=Q R$. Suppose $A = \\left[a_{1} a_{2}\\right]$, by orthonormalization, we have:  $$\n",
    "\\left[a_{1} a_{2}\\right]=\\left[\\begin{array}{l}\n",
    "q_{1} q_{2}\n",
    "\\end{array}\\right]\\left[\\begin{array}{ll}\n",
    "a_{1}^{T} q_{1} & a_{2}^{T} q_{1} \\\\\n",
    "\\underbrace{a_{1}^{T}}_{=0} q_{2} & a_{2}^{T} q_{2}\n",
    "\\end{array}\\right]\n",
    "$$. $R$ is a upper matrix. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
