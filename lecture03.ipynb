{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3: Multiplication and Inverse Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n"  ,
    "Lecture video: https://www.youtube.com/watch?v=FX4C-JpTFgY&list=PLE7DDD91010BC51F8&index=4\n"  ,
    "Chinese note: https://nbviewer.jupyter.org/github/zlotus/notes-linear-algebra/blob/master/chapter03.ipynb  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiplication  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four ways to calculate the product of two matrices. \n",
    "* Inner product of row vector and column vector: Suppose there is $m\\times n$ matrix $A$ and $n\\times p$matrix $B$ (The number of A's columns must be equal to the number of B's rows). Their product $AB=C$, $C$ is a $m\\times p$ matrix. For C's $i$th row and $j$th column element $c_{ij}$, it is calculated as:\n",
    "\n",
    "$$c_{ij}=row_i\\cdot column_j=\\sum_{k=1}^{k=n}a_{ik}b_{kj} $$\n",
    "\n",
    "$a_{ik}$ is the $i$th row $k$th column element of matrix $A$, $b_{kj}$ is the $k$th row $j$th column element of matrix $B$. \n",
    "\n",
    "In other words, $c_{ij}$ is the dot product of $i$th row of matrix $A$ and $j$th column of matrix $B$. \n",
    "\n",
    "$$\\left[\\begin{matrix} &\\vdots& \\cr & row_i& \\cr &\\vdots& \\end{matrix}\\right] \\left[\\begin{matrix} &\\dots&  column_j& \\dots \\end{matrix}\\right] = \\left[\\begin{matrix} &\\vdots& \\cr \\dots & c_{ij}& \\dots \\cr &\\vdots& \\end{matrix}\\right] $$\n",
    "\n",
    "* Multiply by columns: \n",
    "$$\\left[\\begin{matrix} A_{col_1}& A_{col_2}&\\dots & A_{col_n}\\end{matrix}\\right] \\left[\\begin{matrix} \\dots&  b_{1j}& \\dots \\cr \\dots&  b_{2j}&\\dots& \\cr \\dots& \\vdots& \\dots \\cr \\dots&  b_{nj}& \\dots\\end{matrix}\\right] = \\left[\\begin{matrix}\\dots &(b_{1j}A_{col_1} + b_{2j}A_{col_2} + \\dots + b_{nj}A_{col_n}) & \\dots \\end{matrix}\\right] $$\n",
    "\n",
    "The above operation is multiplying $A$ with $j$th column of $B$, and the result is $j$th column of $C$. That is to say, $j$th column of $C$ is the linear combination of $A$'s column vectors with $B$'s $j$th column as the coefficient. \n",
    "\n",
    "$$C_j = b_{1j}A_{col_1} + b_{2j}A_{col_2} + \\dots + b_{nj}A_{col_n} $$\n",
    "\n",
    "* Multiply by rows: \n",
    "$$\\left[\\begin{matrix} \\vdots & \\vdots & \\vdots  & \\vdots \\cr a_{i1} & a_{i2} & \\dots & a_{in} \\cr \\vdots & \\vdots & \\vdots  & \\vdots \\end{matrix}\\right] \\left[\\begin{matrix} B_{row_1} \\cr B_{row_2}\\cr \\vdots \\cr B_{row_n} \\end{matrix}\\right] = \\left[\\begin{matrix}\\vdots \\cr (a_{i1}B_{row_1} + a_{i2}B_{row_2} + \\dots + a_{in}B_{row_n})\\cr  \\vdots \\end{matrix}\\right]$$\n",
    "\n",
    "The above operation is multiplying $B$ with $i$th row of $A$, and the result is the $i$th row of $C$. In other words, the $i$th row of $C$ is the linear combination of $B$'s row vectors with $A$'s $i$th row as the coefficients. \n",
    "\n",
    "$$C_i = a_{i1}B_{row_1} + a_{i2}B_{row_2} + \\dots + a_{in}B_{row_n} $$\n",
    "\n",
    "* Column multiply row: \n",
    "\n",
    "$$\\left[\\begin{matrix} A_{col_1}& A_{col_2}&\\dots & A_{col_n}\\end{matrix}\\right] \\left[\\begin{matrix} B_{row_1} \\cr B_{row_2}\\cr \\vdots \\cr B_{row_n} \\end{matrix}\\right] = A_{col_1}B_{row_1} + A_{col_2}B_{row_2}+\\dots+A_{col_n}B_{row_n} $$\n",
    "\n",
    "$A_{col_i}B_{row_i}$ is a $m\\times1$ vector multiply a $1\\times p$ vector, the result is a $m\\times p$ matirx. $C$ is the sum of all the $m\\times p$ matirx. \n",
    "\n",
    "* Block matrix: \n",
    "\n",
    "$$\\left[\\begin{array} {c|c} A_1 & A_2 \\cr \\hline A_3 & A_4 \\end{array}\\right] \\left[\\begin{array} {c|c} B_1 & B_2 \\cr \\hline B_3 & B_4 \\end{array}\\right] = \\left[\\begin{array} {c|c} A_1B_1 + A_2B_3 & A_1B_2+A_2B_4 \\cr \\hline A_3B_1 + A_4B_3& A_3  B_2 + A_4B_4 \\end{array}\\right]$$\n",
    "\n",
    "In some cases, it will simplifying the calculation by using block matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse (Square matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all the square matrix have inverse. If the inverse exists, $A^{-1}A=I = AA^{-1}$. It should be noted that $A^{-1}A=I = AA^{-1}$ is for square matrix. For rectangular matrix, the equation does not hold because the shape of identity matrix is different. \n",
    "\n",
    "Those matrix who have inverse are also called invertible or nonsingular. Let's see an singular (noninvertible) matrix: $A=\\left[\\begin{matrix}1 & 3\\cr 2 & 6 \\end{matrix}\\right]$. In later lecture about determinant of matrix, we will see that the determinant of the matrix is 0. \n",
    "\n",
    "If we multiply $A$ with a matrix $B$, $AB=C$, then each column of the result matrix $C$ should be certain times of $\\left[\\begin{matrix}1  \\cr 2  \\end{matrix}\\right]$. Therefore, the result can not be identity matrix $I$. That means that $A$ has no inverse. \n",
    "\n",
    "Here gives another way, if $A$ multiply any nonzero vector producing $0$, then $A$ is noninvertible. In other words, see if $Ax=0$ has solutions. Taking the above matrix as an example, since there exists a vecotor  $\\left[\\begin{matrix}-3  \\cr 1  \\end{matrix}\\right]$ such that $\\left[\\begin{matrix}1 & 3\\cr 2 & 6 \\end{matrix}\\right]\\left[\\begin{matrix}-3  \\cr 1  \\end{matrix}\\right]=0 $, therefore $A$ has no inverse. \n",
    "\n",
    "The proof is as follows:\n",
    "\n",
    "Suppose there exists nonzero vector $x$ such that $Ax=0$, and $A$ has inverse $A^{-1}$, then $A^{-1}Ax = 0$, $x=0$, which contrasts with the fact that $x$ is nonzero. \n",
    "\n",
    "Next we gives a matrix that has inverse $A=\\left[\\begin{matrix}1 & 3\\cr 2 & 7 \\end{matrix}\\right]$, and introduces Gauss-Jordan method to calculate the inverse of the matrix, $\\left[\\begin{matrix}1 & 3\\cr 2 & 7 \\end{matrix}\\right] \\left[\\begin{matrix}a & b\\cr c & d \\end{matrix}\\right] = \\left[\\begin{matrix}1 & 0\\cr 0 & 1 \\end{matrix}\\right]$. \n",
    "\n",
    "If we use the idea of column vector multiplication, we can say that $A$ mutiply $A^{-1}$'s $j$th column produces $I$'s $j$th column. Then we can get the following equations:\n",
    "\n",
    "$$\\begin{cases} \\left[\\begin{matrix} 1 & 3\\cr 2 & 7 \\end{matrix}\\right] \\left[\\begin{matrix} a \\cr b \\end{matrix}\\right] =  \\left[\\begin{matrix} 1 \\cr 0 \\end{matrix}\\right] \\\\ \\left[\\begin{matrix} 1 & 3\\cr 2 & 7 \\end{matrix}\\right]\\left[ \\begin{matrix} c \\cr d \\end{matrix}\\right] =  \\left[\\begin{matrix} 0 \\cr 1 \\end{matrix}\\right]\\end{cases}$$. \n",
    "We can solve the equations separately to get $a, b, c, d$. But Gauss-Jordan method can deal with all the equations at one time. \n",
    "\n",
    "* First build a matrix as this:\n",
    "\n",
    "$$\\left[\\begin{array} {c|c} A &I \\end{array}\\right] = \\left[\\begin{array} {cc|cc} 1&3 &1&0 \\cr 2&7 &0&1 \\end{array}\\right]$$\n",
    "\n",
    "* Turning the left matrix $A$ into identity $I$ by elimination:\n",
    "\n",
    "$$\\left[\\begin{array} {cc|cc} 1&3 &1&0 \\cr 2&7 &0&1 \\end{array}\\right] \\xrightarrow{row_2-2row_1} \\left[\\begin{array} {cc|cc} 1&3 &1&0 \\cr 0&1 &-2&1 \\end{array}\\right]\\xrightarrow{row_1-3row_2}\\left[\\begin{array} {cc|cc} 1&0 &7&-3 \\cr 0&1 &-2&1 \\end{array}\\right]$$\n",
    "\n",
    "* Then $\\left[\\begin{array} {c|c} A &I \\end{array}\\right]$ has been turned into $\\left[\\begin{array} {c|c} I & A^{-1} \\end{array}\\right]$. \n",
    "\n",
    "Why is this? \n",
    "\n",
    "The basic idea of Gauss-Jordan method is manipulating $A$ by using elimination matrix $E$, $E \\left[\\begin{array} {c|c} A &I \\end{array}\\right]$. After completing all the steps, for the left part we have $EA=I$, for the right part we have $EI=E$, which finally produces $\\left[\\begin{array} {c|c} I &E \\end{array}\\right]$. Because $EA=I$, thus $E$ is the inverse of matrix $A$. The $I$ on the right part is just to record each elimination operation, when all the elimination is done, $A^{-1}$ is just the elimination matrix $E$. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
